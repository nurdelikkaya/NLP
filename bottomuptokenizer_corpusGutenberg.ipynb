{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Corpus Name and Selection Reason**\n",
        "\n",
        "I chose the Gutenberg corpus which contains a collection of various classical literary texts. Since it would be a nice challenge to tokenize old English with its uncommon vocabulary and different genres with varied sentence structures. \n",
        "        \n",
        "I implemented a bottom-up tokenizer using Byte Pair Encoding since there are words in the corpus that are uncommon to me, thus it would be more efficient than creating a top-down tokenizer.\n",
        "        \n",
        "Below is a step-by-step breakdown of the design:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bottom-up Tokenizer for Gutenberg Corpus using BPE (Byte Pair Encoding)\n",
        "\n",
        "# Import required libraries\n",
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "# Download the selected corpus if not already available\n",
        "corpus_name = 'gutenberg'\n",
        "nltk.download(corpus_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The clean\\_corpus() function takes the raw text and splits it into individual tokens using regex**\n",
        "    \n",
        "*   Words, contractions, and possessives (e.g., \"sister's\", \"don't\", \"it's\").\n",
        "*   Numbers (e.g., \"1865\").\n",
        "*   Punctuation (e.g., commas, periods, and quotes).\n",
        "*   After splitting, the tokens are processed to ensure all alphabetic tokens are converted to lowercase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Clean and tokenize the corpus\n",
        "def clean_corpus(corpus_raw):\n",
        "    \"\"\"\n",
        "    Tokenize the corpus using regex to match words, contractions, possessives, \n",
        "    numbers, punctuation, and hyphenated words. Convert alphabetic tokens to lowercase.\n",
        "\n",
        "    Args:\n",
        "        corpus_raw (str): The raw text of the corpus.\n",
        "\n",
        "    Returns:\n",
        "        tokens (list): List of cleaned and tokenized words.\n",
        "    \"\"\"\n",
        "    # Regex pattern to match contractions and possessives as one token\n",
        "    pattern = r\"\\b\\w+(?:[-']\\w+)*\\b|[^\\w\\s]\"\n",
        "\n",
        "    # Match words, contractions, possessives, punctuation, and hyphenated words\n",
        "    tokens = re.findall(pattern, corpus_raw)\n",
        "\n",
        "    # Lowercase alphabetic tokens or those containing hyphens or apostrophes\n",
        "    tokens = [token.lower() if any(c.isalpha() for c in token) else token for token in tokens]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The get\\_vocab() function** counts the frequency of each token in the corpus using Pythonâ€™s Counter class. This allows the BPE algorithm to identify frequently co-occurring tokens for merging.\n",
        "\n",
        "\n",
        "**The get\\_stats() function** identifies frequent adjacent token pairs with the highest frequency in the text, providing input for the next merge step.\n",
        "\n",
        "\n",
        "**The merge\\_pair() function** takes the most frequent adjacent token pair and merges them into a single token. This helps reduce the overall vocabulary size and improves efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. BPE Helper Functions\n",
        "def get_vocab(corpus):\n",
        "    \"\"\"\n",
        "    Create a frequency dictionary of tokens from the corpus.\n",
        "\n",
        "    Args:\n",
        "        corpus (list): List of tokenized words.\n",
        "\n",
        "    Returns:\n",
        "        vocab (Counter): Frequency dictionary of tokens.\n",
        "    \"\"\"\n",
        "    return Counter(corpus)\n",
        "\n",
        "def get_stats(vocab):\n",
        "    \"\"\"\n",
        "    Calculate the frequency of adjacent token pairs in the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        vocab (Counter): Frequency dictionary of tokens.\n",
        "\n",
        "    Returns:\n",
        "        pairs (Counter): Frequency dictionary of adjacent token pairs.\n",
        "    \"\"\"\n",
        "    pairs = Counter()\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[(symbols[i], symbols[i + 1])] += freq\n",
        "    return pairs\n",
        "\n",
        "def merge_pair(pair, vocab):\n",
        "    \"\"\"\n",
        "    Merge the most frequent adjacent token pair in the vocabulary.\n",
        "\n",
        "    Args:\n",
        "        pair (tuple): The pair of adjacent tokens to be merged.\n",
        "        vocab (Counter): Frequency dictionary of tokens.\n",
        "\n",
        "    Returns:\n",
        "        merged_vocab (dict): Updated vocabulary with merged token pair.\n",
        "    \"\"\"\n",
        "    merged_vocab = {}\n",
        "    bigram = ' '.join(pair)\n",
        "    replacement = ''.join(pair)\n",
        "\n",
        "    for word in vocab:\n",
        "        new_word = word.replace(bigram, replacement)\n",
        "        merged_vocab[new_word] = vocab[word]\n",
        "\n",
        "    return merged_vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The my_tokenizer function** applies Byte Pair Encoding (BPE) to iteratively merge the most frequent adjacent token pairs in a given corpus, reducing the vocabulary size while preserving token structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jYyH_qz3Lii"
      },
      "outputs": [],
      "source": [
        "# 4. Tokenizer function using BPE\n",
        "def my_tokenizer(corpus, num_merges):\n",
        "    \"\"\"\n",
        "    Tokenize the corpus using Byte Pair Encoding (BPE) by iteratively merging \n",
        "    the most frequent token pairs.\n",
        "\n",
        "    Args:\n",
        "        corpus (list): List of tokenized words.\n",
        "        num_merges (int): Number of merges to perform during BPE.\n",
        "\n",
        "    Returns:\n",
        "        list: Final list of tokenized words after BPE merges.\n",
        "    \"\"\"\n",
        "    vocab = get_vocab(corpus)\n",
        "\n",
        "    for i in range(num_merges):\n",
        "        pairs = get_stats(vocab)\n",
        "        if not pairs:\n",
        "            break\n",
        "\n",
        "        # Select the most frequent pair to merge\n",
        "        best_pair = max(pairs, key=pairs.get)\n",
        "        vocab = merge_pair(best_pair, vocab)\n",
        "\n",
        "        print(f\"Step {i + 1}: Merged {best_pair}\")\n",
        "\n",
        "    # Return the final list of merged tokens\n",
        "    return list(vocab.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Main code to run the tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Main code to run the tokenizer\n",
        "print(\"Loading and cleaning the Gutenberg corpus...\")\n",
        "\n",
        "# Load the corpus and clean it\n",
        "corpus_raw = load_corpus()\n",
        "tokens = clean_corpus(corpus_raw)\n",
        "\n",
        "# Set the number of merges for BPE, decided 50 arbitrarily\n",
        "num_merges = 50\n",
        "\n",
        "# Tokenize the corpus using BPE\n",
        "my_tokenized_list = my_tokenizer(tokens, num_merges)\n",
        "\n",
        "# Display the final list of tokens\n",
        "print(\"Tokenization complete. Final tokenized list:\")\n",
        "print(my_tokenized_list)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
